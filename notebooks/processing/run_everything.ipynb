{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run full epistasis pipeline\n",
        "\n",
        "**Single config:** Sources, tools, and per-tool batch sizes live in `notebooks/processing/pipeline_config.py`. Edit that file to add sources or change tools. Output layout: `{output_base}/{source}/{model_key}.db` and `{output_base}/sheets/epistasis_metrics_{tool}.parquet`.\n",
        "\n",
        "**Execution (one tool at a time):** For each conda env (main, evo2, alphagenome), we run all sources (okgp first) for that env's tools. cov_inv is fitted from sources whose name contains \"okgp\" and used for Mahalanobis (epi_mahal, etc.) on all other sources. SpliceAI runs only for sources containing \"fas\".\n",
        "\n",
        "**Notebook flow:**  \n",
        "1. **Config** — Import from `pipeline_config`; set `ENV_PROFILE` for this kernel (main / evo2 / alphagenome).  \n",
        "2. **Step 1 (Embed)** — Run embeddings for the current env's tools over all sources (by-tool: one model load per tool).  \n",
        "3. **Step 2 (cov_inv)** — Compute cov_inv from okgp sources only.  \n",
        "4. **Step 3 (Sheets)** — Recompute metrics with cov_inv; save one parquet per tool to `output_base/sheets/`.\n",
        "\n",
        "**On a cluster (Lambda, 8× A100):** Run once per env, then metrics:\n",
        "```bash\n",
        "bash scripts/run_pipeline_cluster.sh 2>&1 | tee pipeline.log\n",
        "```\n",
        "In another terminal, monitor progress and GPU: `bash scripts/monitor_pipeline.sh`. Progress is written to `output_base/pipeline_status.json` (or `PIPELINE_STATUS_FILE`). To verify GPU and wrappers before a full run: `bash scripts/run_pipeline_cluster.sh --quick-test --env-profile main`. Or per profile: `--env-profile main`, then evo2, then alphagenome; then `--phase metrics`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Config: from pipeline_config + paper_data_config\n",
        "# ---------------------------------------------------------------------------\n",
        "# Edit pipeline_config.py to change sources, tools, batch_size per tool.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "ROOT = Path.cwd()\n",
        "for _ in range(4):\n",
        "    if (ROOT / \"notebooks\" / \"paper_data_config.py\").exists():\n",
        "        break\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from notebooks.processing.pipeline_config import (\n",
        "    SOURCE_COL, ID_COL, SOURCE_MODEL_MAP, COV_INV_SOURCE_NAMES,\n",
        "    get_single_dataframe_path, resolve_sources, get_batch_size,\n",
        ")\n",
        "from notebooks.processing.process_epistasis import get_model_keys_for_env\n",
        "from notebooks.paper_data_config import EPISTASIS_PAPER_ROOT, data_dir, embeddings_dir\n",
        "\n",
        "# Which env this kernel is running in (main | evo2 | alphagenome). Step 1 runs only this env's tools.\n",
        "ENV_PROFILE = \"main\"\n",
        "MODEL_KEYS = get_model_keys_for_env(ENV_PROFILE)\n",
        "OUTPUT_BASE = embeddings_dir()\n",
        "SHEETS_DIR = OUTPUT_BASE / \"sheets\"\n",
        "SPLICEAI_MODEL_DIR = None  # or set path / env OPENSPLICEAI_MODEL_DIR\n",
        "\n",
        "print(f\"ENV_PROFILE={ENV_PROFILE!r} -> MODEL_KEYS={MODEL_KEYS}\")\n",
        "print(f\"Data root: {EPISTASIS_PAPER_ROOT}\")\n",
        "print(f\"Output base: {OUTPUT_BASE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 1: Embed (one tool at a time over all sources); one .db per model per source\n",
        "# ---------------------------------------------------------------------------\n",
        "import logging\n",
        "import pandas as pd\n",
        "from notebooks.processing.process_epistasis import run_sources_by_tool, run_from_single_dataframe\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "batch_size_by_model = {k: get_batch_size(k) for k in MODEL_KEYS}\n",
        "\n",
        "single_path = get_single_dataframe_path(data_dir)\n",
        "if single_path is not None:\n",
        "    df_all = pd.read_csv(single_path, sep=None, engine=\"python\")\n",
        "    run_from_single_dataframe(\n",
        "        df_all,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        source_col=SOURCE_COL,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        source_model_map=SOURCE_MODEL_MAP,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=16,\n",
        "        batch_size_by_model=batch_size_by_model,\n",
        "        use_by_tool=True,\n",
        "    )\n",
        "    SOURCE_NAMES = df_all[SOURCE_COL].dropna().astype(str).unique().tolist()\n",
        "else:\n",
        "    sources = resolve_sources(data_dir)\n",
        "    df_all = None\n",
        "    SOURCE_NAMES = [name for name, _ in sources]\n",
        "    run_sources_by_tool(\n",
        "        sources,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        source_model_map=SOURCE_MODEL_MAP,\n",
        "        spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=16,\n",
        "        batch_size_by_model=batch_size_by_model,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 2: Compute cov_inv from rows whose source is in COV_INV_SOURCE_NAMES (see pipeline_config)\n",
        "# ---------------------------------------------------------------------------\n",
        "from notebooks.processing.process_epistasis import compute_cov_inv, FULL_MODEL_CONFIG\n",
        "\n",
        "if df_all is not None:\n",
        "    src = df_all[SOURCE_COL].astype(str)\n",
        "    df_subset = df_all[src.isin(COV_INV_SOURCE_NAMES)]\n",
        "    okgp_sources = [s for s in src.unique() if s in COV_INV_SOURCE_NAMES]\n",
        "else:\n",
        "    sources = resolve_sources(data_dir)\n",
        "    okgp_sources = [n for n, _ in sources if n in COV_INV_SOURCE_NAMES]\n",
        "    dfs = []\n",
        "    for name, path in sources:\n",
        "        if name not in COV_INV_SOURCE_NAMES:\n",
        "            continue\n",
        "        d = pd.read_csv(path, sep=None, engine=\"python\")\n",
        "        d[SOURCE_COL] = name\n",
        "        dfs.append(d)\n",
        "    df_subset = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "if len(df_subset) == 0:\n",
        "    raise ValueError(\"No rows with source in COV_INV_SOURCE_NAMES=%s; need data for cov_inv.\" % COV_INV_SOURCE_NAMES)\n",
        "model_keys_for_cov = [\n",
        "    k for k in FULL_MODEL_CONFIG\n",
        "    if any((OUTPUT_BASE / src / f\"{k}.db\").exists() for src in okgp_sources)\n",
        "]\n",
        "if not model_keys_for_cov:\n",
        "    model_keys_for_cov = MODEL_KEYS\n",
        "\n",
        "cov_inv_by_model = compute_cov_inv(\n",
        "    OUTPUT_BASE,\n",
        "    source_df=df_subset,\n",
        "    source_col=SOURCE_COL,\n",
        "    id_col=ID_COL,\n",
        "    model_keys=model_keys_for_cov,\n",
        "    method=\"ledoit_wolf\",\n",
        "    show_progress=True,\n",
        ")\n",
        "print(\"cov_inv for models:\", list(cov_inv_by_model.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 3: Recompute metrics with cov_inv → one sheet (parquet) per tool\n",
        "# ---------------------------------------------------------------------------\n",
        "from notebooks.processing.process_epistasis import recompute_metrics_with_cov_inv\n",
        "\n",
        "if df_all is None:\n",
        "    single_path = get_single_dataframe_path(data_dir)\n",
        "    if single_path is not None:\n",
        "        df_full = pd.read_csv(single_path, sep=None, engine=\"python\")\n",
        "    else:\n",
        "        sources = resolve_sources(data_dir)\n",
        "        dfs = []\n",
        "        for name, path in sources:\n",
        "            d = pd.read_csv(path)\n",
        "            d[SOURCE_COL] = name\n",
        "            dfs.append(d)\n",
        "        df_full = pd.concat(dfs, ignore_index=True)\n",
        "else:\n",
        "    df_full = df_all\n",
        "if SOURCE_COL not in df_full.columns or ID_COL not in df_full.columns:\n",
        "    raise ValueError(f\"DataFrame must have {SOURCE_COL!r} and {ID_COL!r}\")\n",
        "\n",
        "metrics_by_tool = recompute_metrics_with_cov_inv(\n",
        "    OUTPUT_BASE,\n",
        "    df_full,\n",
        "    cov_inv_by_model,\n",
        "    source_col=SOURCE_COL,\n",
        "    model_keys=list(cov_inv_by_model),\n",
        "    id_col=ID_COL,\n",
        "    spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "SHEETS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "for tool, tbl in metrics_by_tool.items():\n",
        "    out = SHEETS_DIR / f\"epistasis_metrics_{tool}.parquet\"\n",
        "    tbl.to_parquet(out, index=False)\n",
        "    print(f\"  {tool}: {tbl.shape[0]} rows -> {out.name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Downstream: cov, cov_inv = cov_inv_by_model[\"nt500_multi\"]; sheets in SHEETS_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: null cov packs are already in OUTPUT_BASE/null_cov/{model_key}_pack.npz"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}