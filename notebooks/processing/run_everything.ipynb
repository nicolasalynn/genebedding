{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run full epistasis pipeline\n",
        "\n",
        "**Step 1:** Run the whole dataframe. All double-variant IDs are processed; embeddings are stored under `{output_base}/{source}/{model_key}.db` (split by the `source` column). Null is processed first so null covariance is saved and used for Mahalanobis metrics on non-null sources.\n",
        "\n",
        "**Step 2:** Compute cov_inv from a subset (e.g. null only). Pass a subset dataframe; `compute_cov_inv` uses its sources and, if present, its epistasis_ids to fit covariance per model. Returns `{model_key: (cov, cov_inv)}`.\n",
        "\n",
        "**Step 3:** Recompute metrics with that cov_inv and get a **series of tables**: one table per model (tool). Each table has the same structure (epistasis_id, source, len_WT_M1, epi_R_raw, epi_mahal, …); only the tool differs. No re-embedding; metrics are recomputed from the new DBs. Save each table separately (e.g. one parquet per tool) for downstream use.\n",
        "\n",
        "Set paths and options in the config cell, then run Step 1, Step 2, and Step 3.\n",
        "\n",
        "**Environment control:** Different models need different conda/envs (e.g. AlphaGenome needs JAX, Evo2 its own stack). Set **ENV_PROFILE** in the config cell and run this notebook in the matching environment:\n",
        "- `\"alphagenome\"` — run only AlphaGenome (use AlphaGenome env)\n",
        "- `\"evo2\"` — run only Evo2 (use Evo2 env)\n",
        "- `\"main\"` — run all other models (shared env; excludes alphagenome & evo2)\n",
        "- `\"all\"` — run every model (only if all deps in one env)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Config: paths and options\n",
        "# ---------------------------------------------------------------------------\n",
        "# Run with project root as current working directory (so \"from notebooks.processing.process_epistasis\" works).\n",
        "# All data (embeddings, CSVs) lives under EPISTASIS_PAPER_ROOT. Override with env EPISTASIS_PAPER_ROOT.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure repo root is on path so \"notebooks\" can be imported\n",
        "ROOT = Path.cwd()\n",
        "for _ in range(4):\n",
        "    if (ROOT / \"notebooks\" / \"paper_data_config.py\").exists():\n",
        "        break\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from notebooks.processing.process_epistasis import get_model_keys_for_env\n",
        "from notebooks.paper_data_config import EPISTASIS_PAPER_ROOT, data_dir, embeddings_dir\n",
        "\n",
        "# Environment profile: which models to run in *this* environment.\n",
        "#   \"alphagenome\" -> only AlphaGenome (use in AlphaGenome conda env)\n",
        "#   \"evo2\"       -> only Evo2 (use in Evo2 conda env)\n",
        "#   \"main\"       -> all models except alphagenome & evo2 (shared env)\n",
        "#   \"all\"        -> every model (only if all deps in one env)\n",
        "ENV_PROFILE = \"main\"  # <-- single point of control: change per environment\n",
        "MODEL_KEYS = get_model_keys_for_env(ENV_PROFILE)\n",
        "print(f\"ENV_PROFILE={ENV_PROFILE!r} -> MODEL_KEYS={MODEL_KEYS}\")\n",
        "print(f\"Data root: {EPISTASIS_PAPER_ROOT}\")\n",
        "\n",
        "# Input mode: single dataframe with 'source' column, or list of (source_name, path) per source.\n",
        "# - If USE_SINGLE_DATAFRAME is True: load one table (e.g. epistasis_aggregated.csv) with columns\n",
        "#   epistasis_id and SOURCE_COL; storage dirs are split by value in SOURCE_COL: {output_base}/{source}/{model}.db.\n",
        "# - If False: use SOURCES list below (one CSV per source).\n",
        "USE_SINGLE_DATAFRAME = False\n",
        "SINGLE_DATAFRAME_PATH = data_dir() / \"epistasis_aggregated.csv\"  # used when USE_SINGLE_DATAFRAME is True\n",
        "# Resolve relative paths (e.g. \"all_pairs_combined.csv\") against data_dir so they work regardless of cwd\n",
        "if USE_SINGLE_DATAFRAME and SINGLE_DATAFRAME_PATH is not None:\n",
        "    p = Path(SINGLE_DATAFRAME_PATH)\n",
        "    if not p.is_absolute() and not p.exists():\n",
        "        p = data_dir() / p\n",
        "        if p.exists():\n",
        "            SINGLE_DATAFRAME_PATH = p\n",
        "SOURCE_COL = \"source\"  # column in the single dataframe that defines the storage subdirectory per row\n",
        "\n",
        "# (source_name, path_to_csv). Used when USE_SINGLE_DATAFRAME is False. Paths under paper data root/data/\n",
        "SOURCES = [\n",
        "    (\"null\", data_dir() / \"null_epistasis.csv\"),\n",
        "    (\"fas_analysis\", data_dir() / \"fas_subset.csv\"),\n",
        "    (\"mst1r_analysis\", data_dir() / \"mst1r_subset.csv\"),\n",
        "    (\"kras\", data_dir() / \"kras_subset.csv\"),\n",
        "    # (\"tcga_analysis\", data_dir() / \"tcga_subset_doubles.csv\"),\n",
        "    # (\"okgp_analysis\", data_dir() / \"okgp_subset_clean.csv\"),\n",
        "]\n",
        "\n",
        "OUTPUT_BASE = embeddings_dir()\n",
        "ID_COL = \"epistasis_id\"\n",
        "BATCH_SIZE = 8  # add_epistasis_metrics batch_size (e.g. 8 => 32 sequences per batch)\n",
        "\n",
        "# Optional: which models run on which sources. None = use MODEL_KEYS for all, with SpliceAI only on splicing sources.\n",
        "# Example: {\"null\": [\"nt500_multi\", \"convnova\"], \"fas_analysis\": [\"nt500_multi\", \"convnova\", \"spliceai\"]}\n",
        "SOURCE_MODEL_MAP = None\n",
        "\n",
        "# Optional: list of directories to search for existing embeddings. For each epistasis_id, copy into new DB if found.\n",
        "# If EMBEDDING_LOOKUP_FLAT is False: each base has base/source_name/model_key.db (same as OUTPUT_BASE).\n",
        "# If True: each base has .db files directly (base/model_key.db), e.g. /path/embeddings/nt500_multi.db.\n",
        "# Example: [Path(\"/path1/embeddings\"), Path(\"/path2/embeddings\")]\n",
        "EMBEDDING_LOOKUP_BASES = None\n",
        "EMBEDDING_LOOKUP_FLAT = True  # set True when each lookup dir has alphagenome.db, convnova.db, ... directly\n",
        "\n",
        "# OpenSpliceAI checkpoint dir for SpliceAI (or set env OPENSPLICEAI_MODEL_DIR)\n",
        "SPLICEAI_MODEL_DIR = None  # e.g. \"/path/to/openspliceai-mane/10000nt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 1: Process all sources (null first); one .db per model per source\n",
        "# ---------------------------------------------------------------------------\n",
        "# Either run from a single dataframe (storage split by SOURCE_COL) or from SOURCES list.\n",
        "import logging\n",
        "import pandas as pd\n",
        "from notebooks.processing.process_epistasis import run_sources, run_from_single_dataframe\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "if USE_SINGLE_DATAFRAME and SINGLE_DATAFRAME_PATH is not None and Path(SINGLE_DATAFRAME_PATH).exists():\n",
        "    df_all = pd.read_csv(SINGLE_DATAFRAME_PATH)\n",
        "    run_from_single_dataframe(\n",
        "        df_all,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        source_col=SOURCE_COL,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        source_model_map=SOURCE_MODEL_MAP,\n",
        "        embedding_lookup_bases=EMBEDDING_LOOKUP_BASES,\n",
        "        embedding_lookup_flat=EMBEDDING_LOOKUP_FLAT,\n",
        "        spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    # For Step 2: source names in same order (null first)\n",
        "    _unique = df_all[SOURCE_COL].dropna().astype(str).unique().tolist()\n",
        "    SOURCE_NAMES = [s for s in _unique if s == \"null\"] + [s for s in _unique if s != \"null\"]\n",
        "else:\n",
        "    df_all = None  # so Step 2 can check and use a small df for source subset\n",
        "    run_sources(\n",
        "        SOURCES,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        source_model_map=SOURCE_MODEL_MAP,\n",
        "        embedding_lookup_bases=EMBEDDING_LOOKUP_BASES,\n",
        "        embedding_lookup_flat=EMBEDDING_LOOKUP_FLAT,\n",
        "        spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    SOURCE_NAMES = [name for name, _ in SOURCES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 2: Compute cov_inv from a part of the dataframe — returns (cov, cov_inv) directly\n",
        "# ---------------------------------------------------------------------------\n",
        "# Pass a subset of your data (by source). You get back a dict: {model_key: (cov, cov_inv)}.\n",
        "from notebooks.processing.process_epistasis import compute_cov_inv\n",
        "\n",
        "# Define which sources to use (e.g. null only, or null + fas_analysis).\n",
        "# If you ran Step 1 from a single dataframe, filter it; otherwise use a small df with source names.\n",
        "try:\n",
        "    _df = df_all\n",
        "except NameError:\n",
        "    _df = None\n",
        "if _df is not None:\n",
        "    df_subset = _df[_df[SOURCE_COL].isin([\"null\"])]   # e.g. null only\n",
        "else:\n",
        "    df_subset = __import__(\"pandas\").DataFrame({SOURCE_COL: [\"null\"]})  # null only by source name\n",
        "\n",
        "cov_inv_by_model = compute_cov_inv(\n",
        "    OUTPUT_BASE,\n",
        "    source_df=df_subset,\n",
        "    source_col=SOURCE_COL,\n",
        "    id_col=ID_COL,\n",
        "    model_keys=MODEL_KEYS,\n",
        "    method=\"ledoit_wolf\",\n",
        "    show_progress=True,\n",
        ")\n",
        "# Example: get arrays for one model\n",
        "# cov, cov_inv = cov_inv_by_model[\"nt500_multi\"]\n",
        "# cov.shape, cov_inv.shape\n",
        "print(\"Returned cov_inv for models:\", list(cov_inv_by_model.keys()))\n",
        "for k, (c, ci) in cov_inv_by_model.items():\n",
        "    print(f\"  {k}: cov {c.shape}, cov_inv {ci.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 3: Recompute metrics with cov_inv → one table per tool (same structure each)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Uses embeddings already in OUTPUT_BASE; only metric columns are recomputed.\n",
        "# Returns a dict: {model_key: DataFrame}. Each table has the same columns\n",
        "# (epistasis_id, source, len_WT_M1, epi_R_raw, epi_mahal, ...); one table per tool.\n",
        "from notebooks.processing.process_epistasis import recompute_metrics_with_cov_inv\n",
        "\n",
        "# Full dataframe (must have epistasis_id and source). Use df_all from Step 1 or load your table.\n",
        "try:\n",
        "    _df_full = df_all\n",
        "except NameError:\n",
        "    _df_full = None\n",
        "if _df_full is None and USE_SINGLE_DATAFRAME and SINGLE_DATAFRAME_PATH is not None and Path(SINGLE_DATAFRAME_PATH).exists():\n",
        "    _df_full = pd.read_csv(SINGLE_DATAFRAME_PATH)\n",
        "if _df_full is None or len(_df_full) == 0:\n",
        "    raise ValueError(\"Need full dataframe with epistasis_id and source. Run Step 1 first or set SINGLE_DATAFRAME_PATH.\")\n",
        "\n",
        "metrics_by_tool = recompute_metrics_with_cov_inv(\n",
        "    OUTPUT_BASE,\n",
        "    _df_full,\n",
        "    cov_inv_by_model,\n",
        "    source_col=SOURCE_COL,\n",
        "    model_keys=list(cov_inv_by_model),\n",
        "    id_col=ID_COL,\n",
        "    spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    show_progress=True,\n",
        ")\n",
        "# One table per model; same structure (epistasis_id, source, len_WT_M1, epi_mahal, ...)\n",
        "print(\"Series of tables: one per tool, same columns in each\")\n",
        "for tool, tbl in metrics_by_tool.items():\n",
        "    print(f\"  {tool}: {tbl.shape[0]} rows, columns {list(tbl.columns)[:6]}...\")\n",
        "\n",
        "# Optional: save one table per tool (e.g. epistasis_metrics_nt500_multi.parquet)\n",
        "# for tool, tbl in metrics_by_tool.items():\n",
        "#     tbl.to_parquet(data_dir() / f\"epistasis_metrics_{tool}.parquet\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unpack for one model (e.g. for downstream use or add_epistasis_metrics with cov_inv=...)\n",
        "# cov, cov_inv = cov_inv_by_model[\"nt500_multi\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: save cov_inv to .npz if you need it on disk later\n",
        "# from notebooks.processing.process_epistasis import run_covariance_and_save\n",
        "# run_covariance_and_save(OUTPUT_BASE, source_names, model_keys=MODEL_KEYS, out_npz_dir=OUTPUT_BASE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}