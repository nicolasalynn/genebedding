{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run full epistasis pipeline\n",
        "\n",
        "**Step 1:** Run the whole dataframe. All double-variant IDs are processed; embeddings are stored under `{output_base}/{source}/{model_key}.db` (split by the `source` column). Null is processed first so null covariance is saved and used for Mahalanobis metrics on non-null sources.\n",
        "\n",
        "**Step 2:** Pass a part of that dataframe (e.g. null only, or null + one other source). `compute_cov_inv` uses the embeddings in the corresponding source DBs and **returns** `(cov, cov_inv)` directly—no separate save step.\n",
        "\n",
        "Set paths and options in the config cell, then run Step 1 and Step 2.\n",
        "\n",
        "**Environment control:** Different models need different conda/envs (e.g. AlphaGenome needs JAX, Evo2 its own stack). Set **ENV_PROFILE** in the config cell and run this notebook in the matching environment:\n",
        "- `\"alphagenome\"` — run only AlphaGenome (use AlphaGenome env)\n",
        "- `\"evo2\"` — run only Evo2 (use Evo2 env)\n",
        "- `\"main\"` — run all other models (shared env; excludes alphagenome & evo2)\n",
        "- `\"all\"` — run every model (only if all deps in one env)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Config: paths and options\n",
        "# ---------------------------------------------------------------------------\n",
        "# Run with project root as current working directory (so \"from notebooks.process_epistasis\" works).\n",
        "# All data (embeddings, CSVs) lives under EPISTASIS_PAPER_ROOT. Override with env EPISTASIS_PAPER_ROOT.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure repo root is on path so \"notebooks\" can be imported\n",
        "ROOT = Path.cwd()\n",
        "for _ in range(4):\n",
        "    if (ROOT / \"notebooks\" / \"paper_data_config.py\").exists():\n",
        "        break\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from notebooks.process_epistasis import get_model_keys_for_env\n",
        "from notebooks.paper_data_config import EPISTASIS_PAPER_ROOT, data_dir, embeddings_dir\n",
        "\n",
        "# Environment profile: which models to run in *this* environment.\n",
        "#   \"alphagenome\" -> only AlphaGenome (use in AlphaGenome conda env)\n",
        "#   \"evo2\"       -> only Evo2 (use in Evo2 conda env)\n",
        "#   \"main\"       -> all models except alphagenome & evo2 (shared env)\n",
        "#   \"all\"        -> every model (only if all deps in one env)\n",
        "ENV_PROFILE = \"main\"  # <-- single point of control: change per environment\n",
        "MODEL_KEYS = get_model_keys_for_env(ENV_PROFILE)\n",
        "print(f\"ENV_PROFILE={ENV_PROFILE!r} -> MODEL_KEYS={MODEL_KEYS}\")\n",
        "print(f\"Data root: {EPISTASIS_PAPER_ROOT}\")\n",
        "\n",
        "# Input mode: single dataframe with 'source' column, or list of (source_name, path) per source.\n",
        "# - If USE_SINGLE_DATAFRAME is True: load one table (e.g. epistasis_aggregated.csv) with columns\n",
        "#   epistasis_id and SOURCE_COL; storage dirs are split by value in SOURCE_COL: {output_base}/{source}/{model}.db.\n",
        "# - If False: use SOURCES list below (one CSV per source).\n",
        "USE_SINGLE_DATAFRAME = False\n",
        "SINGLE_DATAFRAME_PATH = data_dir() / \"epistasis_aggregated.csv\"  # used when USE_SINGLE_DATAFRAME is True\n",
        "SOURCE_COL = \"source\"  # column in the single dataframe that defines the storage subdirectory per row\n",
        "\n",
        "# (source_name, path_to_csv). Used when USE_SINGLE_DATAFRAME is False. Paths under paper data root/data/\n",
        "SOURCES = [\n",
        "    (\"null\", data_dir() / \"null_epistasis.csv\"),\n",
        "    (\"fas_analysis\", data_dir() / \"fas_subset.csv\"),\n",
        "    (\"mst1r_analysis\", data_dir() / \"mst1r_subset.csv\"),\n",
        "    (\"kras\", data_dir() / \"kras_subset.csv\"),\n",
        "    # (\"tcga_analysis\", data_dir() / \"tcga_subset_doubles.csv\"),\n",
        "    # (\"okgp_analysis\", data_dir() / \"okgp_subset_clean.csv\"),\n",
        "]\n",
        "\n",
        "OUTPUT_BASE = embeddings_dir()\n",
        "ID_COL = \"epistasis_id\"\n",
        "BATCH_SIZE = 8  # add_epistasis_metrics batch_size (e.g. 8 => 32 sequences per batch)\n",
        "\n",
        "# OpenSpliceAI checkpoint dir for SpliceAI (or set env OPENSPLICEAI_MODEL_DIR)\n",
        "SPLICEAI_MODEL_DIR = None  # e.g. \"/path/to/openspliceai-mane/10000nt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 1: Process all sources (null first); one .db per model per source\n",
        "# ---------------------------------------------------------------------------\n",
        "# Either run from a single dataframe (storage split by SOURCE_COL) or from SOURCES list.\n",
        "import logging\n",
        "import pandas as pd\n",
        "from notebooks.process_epistasis import run_sources, run_from_single_dataframe\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "if USE_SINGLE_DATAFRAME and SINGLE_DATAFRAME_PATH is not None and Path(SINGLE_DATAFRAME_PATH).exists():\n",
        "    df_all = pd.read_csv(SINGLE_DATAFRAME_PATH)\n",
        "    run_from_single_dataframe(\n",
        "        df_all,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        source_col=SOURCE_COL,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    # For Step 2: source names in same order (null first)\n",
        "    _unique = df_all[SOURCE_COL].dropna().astype(str).unique().tolist()\n",
        "    SOURCE_NAMES = [s for s in _unique if s == \"null\"] + [s for s in _unique if s != \"null\"]\n",
        "else:\n",
        "    df_all = None  # so Step 2 can check and use a small df for source subset\n",
        "    run_sources(\n",
        "        SOURCES,\n",
        "        output_base=OUTPUT_BASE,\n",
        "        model_keys=MODEL_KEYS,\n",
        "        spliceai_model_dir=SPLICEAI_MODEL_DIR,\n",
        "        id_col=ID_COL,\n",
        "        show_progress=True,\n",
        "        force=False,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    SOURCE_NAMES = [name for name, _ in SOURCES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Step 2: Compute cov_inv from a part of the dataframe — returns (cov, cov_inv) directly\n",
        "# ---------------------------------------------------------------------------\n",
        "# Pass a subset of your data (by source). You get back a dict: {model_key: (cov, cov_inv)}.\n",
        "from notebooks.process_epistasis import compute_cov_inv\n",
        "\n",
        "# Define which sources to use (e.g. null only, or null + fas_analysis).\n",
        "# If you ran Step 1 from a single dataframe, filter it; otherwise use a small df with source names.\n",
        "try:\n",
        "    _df = df_all\n",
        "except NameError:\n",
        "    _df = None\n",
        "if _df is not None:\n",
        "    df_subset = _df[_df[SOURCE_COL].isin([\"null\"])]   # e.g. null only\n",
        "else:\n",
        "    df_subset = __import__(\"pandas\").DataFrame({SOURCE_COL: [\"null\"]})  # null only by source name\n",
        "\n",
        "cov_inv_by_model = compute_cov_inv(\n",
        "    OUTPUT_BASE,\n",
        "    source_df=df_subset,\n",
        "    source_col=SOURCE_COL,\n",
        "    model_keys=MODEL_KEYS,\n",
        "    method=\"ledoit_wolf\",\n",
        "    show_progress=True,\n",
        ")\n",
        "# Example: get arrays for one model\n",
        "# cov, cov_inv = cov_inv_by_model[\"nt500_multi\"]\n",
        "# cov.shape, cov_inv.shape\n",
        "print(\"Returned cov_inv for models:\", list(cov_inv_by_model.keys()))\n",
        "for k, (c, ci) in cov_inv_by_model.items():\n",
        "    print(f\"  {k}: cov {c.shape}, cov_inv {ci.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unpack for one model (e.g. for downstream use or add_epistasis_metrics with cov_inv=...)\n",
        "# cov, cov_inv = cov_inv_by_model[\"nt500_multi\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: save cov_inv to .npz if you need it on disk later\n",
        "# from notebooks.process_epistasis import run_covariance_and_save\n",
        "# run_covariance_and_save(OUTPUT_BASE, source_names, model_keys=MODEL_KEYS, out_npz_dir=OUTPUT_BASE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}